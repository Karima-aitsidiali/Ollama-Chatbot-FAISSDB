import sys
import time
import os, re
import pickle
import faiss
import numpy as np
from colorama import Fore, Style
from sentence_transformers import SentenceTransformer
# Supposons que OllamaAPI, FileProcessor, FilterManager sont correctement import√©s
from ollama_api import OllamaAPI
from Utilitaire.file_processor import FileProcessor
from Utilitaire.filter_manager import FilterManager
from Utilitaire.promptbuilder import PromptBuilder

import traceback # Pour un meilleur d√©bogage

class RAGChatbot:
    
    def __init__(self, ollama_api, chunk_size=384, chunk_overlap=96, faiss_index_file='./vector_store/faiss_index.faiss', metadata_file='./vector_store/metadata.pickle', hashes_file='./vector_store/hashes.pickle'):
        self.ollama_api = ollama_api
        self.embedding_model = SentenceTransformer('BAAI/bge-base-en-v1.5')
        self.file_processor = FileProcessor(chunk_size, chunk_overlap)
        self.dimension = 768  # Correct pour BAAI/bge-base-en-v1.5
        self.MCNoeud = 32
        self.efSearch = 100
        self.efConstruction = 80
        self.faiss_index_file = faiss_index_file
        self.metadata_file = metadata_file
        self.hashes_file = hashes_file

        self.index = self.load_or_initialize_index()
        self.metadata = self.load_or_initialize_metadata() # C'est une liste de dictionnaires
        self.load_processed_hashes()

    def normalize_embedding(self, embedding):
        norm = np.linalg.norm(embedding)
        return embedding / norm if norm > 0 else embedding

    def load_or_initialize_index(self):
        if os.path.exists(self.faiss_index_file):
            # print(f"Chargement de l'index Faiss depuis {self.faiss_index_file}")
            return faiss.read_index(self.faiss_index_file)
        else:
            
            
            index = faiss.IndexHNSWFlat(self.dimension, self.MCNoeud, faiss.METRIC_INNER_PRODUCT)
            index.hnsw.efSearch = self.efSearch
            index.hnsw.efConstruction = self.efConstruction
            return index

    def load_or_initialize_metadata(self):
        if os.path.exists(self.metadata_file):
            #print(f"Chargement des m√©tadonn√©es depuis {self.metadata_file}")
            with open(self.metadata_file, 'rb') as f:
                return pickle.load(f)
        else:
            print("Initialisation d'une nouvelle liste de m√©tadonn√©es.")
            return []

    def load_processed_hashes(self):
        if os.path.exists(self.hashes_file):
            #print(f"Chargement des hashes trait√©s depuis {self.hashes_file}")
            with open(self.hashes_file, 'rb') as f:
                self.file_processor.processed_hashes = pickle.load(f)
        else:
            #print("Initialisation d'un nouvel ensemble de hashes trait√©s.")
            # Assurez-vous que processed_hashes est initialis√© dans FileProcessor si le fichier n'existe pas
            if not hasattr(self.file_processor, 'processed_hashes'):
                 self.file_processor.processed_hashes = set()


    def save_state(self):
        print("Sauvegarde de l'√©tat (index, m√©tadonn√©es, hashes)...")
        os.makedirs(os.path.dirname(self.faiss_index_file), exist_ok=True)
        faiss.write_index(self.index, self.faiss_index_file)
        with open(self.metadata_file, 'wb') as f:
            pickle.dump(self.metadata, f)
        with open(self.hashes_file, 'wb') as f:
            pickle.dump(self.file_processor.processed_hashes, f)
        print("√âtat sauvegard√©.")

    def reset_faiss_database(self):
        """
        Vide compl√®tement la base vectorielle FAISS en supprimant tous les fichiers
        d'index existants et r√©initialise avec un nouvel index vide.
        
        Returns:
            dict: Message de confirmation avec le statut de l'op√©ration
        """
        try:
            print("D√©but de la r√©initialisation de la base vectorielle FAISS...")
            
            # Liste des fichiers √† supprimer
            files_to_remove = [
                self.faiss_index_file,
                self.metadata_file, 
                self.hashes_file
            ]
            
            # Supprimer les fichiers existants s'ils existent
            removed_files = []
            for file_path in files_to_remove:
                if os.path.exists(file_path):
                    try:
                        os.remove(file_path)
                        removed_files.append(file_path)
                        print(f"Fichier supprim√© : {file_path}")
                    except OSError as e:
                        print(f"Erreur lors de la suppression de {file_path} : {e}")
                        raise
            
            # R√©initialiser l'index FAISS avec un nouvel index vide
            print("Cr√©ation d'un nouvel index FAISS vide...")
            self.index = faiss.IndexHNSWFlat(self.dimension, self.MCNoeud, faiss.METRIC_INNER_PRODUCT)
            self.index.hnsw.efSearch = self.efSearch
            self.index.hnsw.efConstruction = self.efConstruction
            
            # R√©initialiser les m√©tadonn√©es
            self.metadata = []
            
            # R√©initialiser les hashes trait√©s
            self.file_processor.processed_hashes = set()
            
            # Supprimer les m√©tadonn√©es de la base SQLite (si n√©cessaire)
            try:
                FilterManager.clear_all_metadata_sqlite()
                print("M√©tadonn√©es SQLite vid√©es")
            except Exception as e:
                print(f"Avertissement : Erreur lors du vidage des m√©tadonn√©es SQLite : {e}")
            
            # Sauvegarder l'√©tat vide
            self.save_state()
            
            result_message = {
                "status": "success",
                "message": "Base vectorielle FAISS r√©initialis√©e avec succ√®s",
                "details": {
                    "removed_files": removed_files,
                    "total_vectors_before": "Inconnu (fichiers supprim√©s)",
                    "total_vectors_after": 0,
                    "metadata_count": 0,
                    "processed_hashes_count": 0
                }
            }
            
            print("R√©initialisation FAISS termin√©e avec succ√®s")
            return result_message
            
        except Exception as e:
            error_message = {
                "status": "error", 
                "message": f"Erreur lors de la r√©initialisation FAISS : {str(e)}",
                "details": {
                    "error_type": type(e).__name__
                }
            }
            print(f"Erreur lors de la r√©initialisation FAISS : {e}")
            traceback.print_exc()
            return error_message

    def ingestion_file(self, base_filename, file_content, departement_id, filiere_id, module_id, activite_id, profile_id, user_id):
        try:
            chunks, file_hash = self.file_processor.process_file(base_filename, file_content)
            if chunks is None:
                print(f"Traitement de {base_filename} annul√© (d√©j√† trait√© ou contenu non valide).")
                if file_hash:
                    raise ValueError(f"Le fichier {base_filename} (hash: {file_hash}) a d√©j√† √©t√© trait√©.")
                else:
                    raise ValueError(f"Le contenu du fichier {base_filename} n'a pas pu √™tre trait√© ou √©tait vide.")
            if not chunks:
                print(f"Aucun chunk extrait de {base_filename}. L'indexation est annul√©e pour ce fichier.")
                return

            embeddings = []
            for chunk_text_content in chunks:
                # Pour BGE, il est recommand√© de ne pas ajouter d'instruction aux documents lors de l'indexation.
                embedding = self.embedding_model.encode([chunk_text_content], normalize_embeddings=True)[0]
                # La normalisation est d√©j√† faite par normalize_embeddings=True
                # normalized_embedding = self.normalize_embedding(embedding) # Plus besoin si normalize_embeddings=True
                embeddings.append(embedding) # Utiliser directement l'embedding normalis√©

            if not embeddings:
                print(f"Aucun embedding n'a pu √™tre g√©n√©r√© pour les chunks de {base_filename}.")
                raise ValueError(f"√âchec de la g√©n√©ration d'embeddings pour {base_filename}.")

            embeddings_np = np.array(embeddings, dtype='float32')
            start_index = self.index.ntotal
            self.index.add(embeddings_np)

            for i, chunk_text_content in enumerate(chunks):
                current_global_chunk_index = start_index + i
                self.metadata.append({
                    "file_hash": file_hash,
                    "original_filename": base_filename,
                    "faiss_index": current_global_chunk_index,
                    "chunk_text": chunk_text_content,
                    # Stocker aussi les filtres pour un acc√®s direct si n√©cessaire, ou compter sur la DB
                    "departement_id": departement_id,
                    "filiere_id": filiere_id,
                    "module_id": module_id,
                    "activite_id": activite_id,
                    "profile_id": profile_id,
                    "user_id": user_id
                })
                FilterManager.insert_metadata_sqlite(
                    base_filename=base_filename, file_hash=file_hash,
                    chunk_index=current_global_chunk_index, chunk_text=chunk_text_content,
                    departement_id=departement_id, filiere_id=filiere_id, module_id=module_id,
                    activite_id=activite_id, profile_id=profile_id, user_id=user_id
                )
            self.save_state()
            print(f"Fichier {base_filename} index√©. {len(chunks)} chunks ajout√©s. L'index Faiss contient maintenant {self.index.ntotal} vecteurs.")
        except ValueError as ve:
            print(f"Erreur de valeur lors de l'indexation de {base_filename} : {str(ve)}")
            raise
        except Exception as e:
            print(f"Erreur inattendue lors de l'indexation de {base_filename} : {str(e)}")
            traceback.print_exc()
            raise

    # --- MMR (Maximal Marginal Relevance) implementation optionnelle ---
    def mmr(self, query_embedding, candidate_embeddings, k=3, lambda_param=0.5):
        """
        Maximal Marginal Relevance (MMR) pour diversifier les chunks s√©lectionn√©s.
        query_embedding: np.array de shape (dim,)
        candidate_embeddings: np.array de shape (n, dim)
        k: nombre de chunks √† retourner
        lambda_param: balance entre pertinence et diversit√© (0.0 = diversit√© max, 1.0 = pertinence max)
        """
        selected = []
        selected_indices = []
        candidate_indices = list(range(len(candidate_embeddings)))
        query_embedding = query_embedding.reshape(1, -1)
        candidate_embeddings = np.array(candidate_embeddings)
        # Similarit√© entre la requ√™te et chaque chunk
        sim_to_query = np.dot(candidate_embeddings, query_embedding.T).flatten()
        for _ in range(min(k, len(candidate_embeddings))):
            if not selected:
                idx = int(np.argmax(sim_to_query))
                selected.append(candidate_embeddings[idx])
                selected_indices.append(idx)
                candidate_indices.remove(idx)
            else:
                max_score = -np.inf
                max_idx = -1
                for idx in candidate_indices:
                    # Pertinence
                    relevance = sim_to_query[idx]
                    # Diversit√© : max similarit√© avec les d√©j√† s√©lectionn√©s
                    diversity = max([np.dot(candidate_embeddings[idx], s) for s in selected])
                    mmr_score = lambda_param * relevance - (1 - lambda_param) * diversity
                    if mmr_score > max_score:
                        max_score = mmr_score
                        max_idx = idx
                selected.append(candidate_embeddings[max_idx])
                selected_indices.append(max_idx)
                candidate_indices.remove(max_idx)
        return selected_indices

    # --- Recherche de contexte avec option MMR (Maximal Marginal Relevance) ---
    def find_relevant_context(self, user_query,
                              departement_id=None, filiere_id=None,
                              top_k=3, similarity_threshold=0.65, use_mmr=False, mmr_lambda=0.5):
        """
        Recherche les chunks les plus pertinents pour une requ√™te utilisateur,
        en utilisant les ID Faiss globaux filtr√©s et en reconstruisant les vecteurs correctement.
        """
        try:
            # Pour BGE, il est souvent recommand√© d'ajouter une instruction aux requ√™tes.
            # Consultez la documentation du mod√®le BGE sp√©cifique.
            # Pour 'BAAI/bge-base-en-v1.5', l'instruction est souvent:
            # "Represent this sentence for searching relevant passages: "
            # Cependant, SentenceTransformer peut le g√©rer implicitement pour certains mod√®les.
            # Si vous utilisez normalize_embeddings=True, la normalisation est d√©j√† faite.
            query_embedding = self.embedding_model.encode([user_query], normalize_embeddings=True)[0]
            # normalized_query = self.normalize_embedding(query_embedding).reshape(1, -1) # Plus besoin
            normalized_query = query_embedding.reshape(1, -1)
        except Exception as e:
            print(f"Erreur lors de la g√©n√©ration de l'embedding de la requ√™te: {e}")
            traceback.print_exc()
            return None

        if not hasattr(self.index, 'ntotal') or self.index.ntotal == 0:
            print("Aucun contexte index√© dans Faiss ou l'index n'est pas correctement initialis√©.")
            return None

        try:
            allowed_faiss_ids = FilterManager.get_allowed_indices(
                departement_id, filiere_id
            )
        except Exception as e:
            print(f"Erreur lors de la r√©cup√©ration des IDs autoris√©s depuis FilterManager: {e}")
            traceback.print_exc()
            return None

        if not allowed_faiss_ids:
            print("Aucun chunk autoris√© trouv√© pour ce contexte acad√©mique et ces filtres.")
            return None

        allowed_faiss_ids_array = np.array(list(allowed_faiss_ids), dtype='int64')
        valid_ids_for_reconstruction = np.array(
            [fid for fid in allowed_faiss_ids_array if 0 <= fid < self.index.ntotal],
            dtype='int64'
        )

        if len(valid_ids_for_reconstruction) == 0:
            print("Aucun ID Faiss autoris√© n'est actuellement valide dans l'index principal.")
            return None

        sub_vectors_list = []
        try:
            # Pour IndexHNSWFlat, les vecteurs sont dans 'storage' qui est un IndexFlat.
            # Si self.index.storage n'existe pas ou est None, il y a un probl√®me avec l'init de l'index HNSW.
            if not hasattr(self.index, 'storage') or self.index.storage is None:
                 print("L'index HNSW ne semble pas avoir de 'storage' valide pour la reconstruction.")
                 return None
            
            reconstruction_source = self.index.storage
            for vector_id in valid_ids_for_reconstruction:
                reconstructed_vec = reconstruction_source.reconstruct(int(vector_id))
                sub_vectors_list.append(reconstructed_vec)

            if not sub_vectors_list:
                print("La liste des vecteurs reconstruits est vide.")
                return None
            sub_vectors = np.array(sub_vectors_list, dtype='float32')

        except AttributeError as ae:
            print(f"Erreur d'attribut lors de la reconstruction (v√©rifiez type d'index): {ae}")
            traceback.print_exc()
            return None
        except Exception as e:
            print(f"Erreur inattendue lors de la reconstruction des vecteurs: {e}")
            traceback.print_exc()
            return None

        if sub_vectors.shape[0] == 0:
            print("Aucun vecteur n'a pu √™tre reconstruit (shape[0] est 0).")
            return None

        try:
            # Le sous-index doit utiliser la m√™me m√©trique que l'index principal si on compare les scores
            # Si l'index HNSW utilise METRIC_INNER_PRODUCT, le sous-index aussi.
            sub_index = faiss.IndexFlatIP(self.dimension)
            sub_index.add(sub_vectors)
        except Exception as e:
            print(f"Erreur lors de la cr√©ation/ajout au sous-index Faiss: {e}")
            traceback.print_exc()
            return None

        try:
            k_search = min(top_k * 5 if use_mmr else top_k, sub_index.ntotal)
            if k_search == 0:
                print("Le sous-index est vide, impossible de rechercher.")
                return None
            distances, local_indices_in_sub = sub_index.search(normalized_query, k_search)
        except Exception as e:
            print(f"Erreur lors de la recherche sur le sous-index Faiss: {e}")
            traceback.print_exc()
            return None

        # R√©cup√©rer les embeddings des candidats pour MMR
        candidate_embeddings = [sub_vectors[i] for i in local_indices_in_sub[0] if i >= 0 and distances[0][list(local_indices_in_sub[0]).index(i)] >= similarity_threshold]
        candidate_indices = [i for i in local_indices_in_sub[0] if i >= 0 and distances[0][list(local_indices_in_sub[0]).index(i)] >= similarity_threshold]

        if use_mmr and candidate_embeddings:
            mmr_indices = self.mmr(normalized_query.flatten(), candidate_embeddings, k=top_k, lambda_param=mmr_lambda)
            selected_indices = [candidate_indices[i] for i in mmr_indices]
        else:
            selected_indices = candidate_indices[:top_k]

        relevant_chunks_texts = []
        for i in selected_indices:
            global_faiss_id = valid_ids_for_reconstruction[i]
            chunk_data = None
            for meta_item in self.metadata:
                if meta_item.get("faiss_index") == global_faiss_id:
                    chunk_data = meta_item
                    break
            if chunk_data and 'chunk_text' in chunk_data:
                relevant_chunks_texts.append(chunk_data['chunk_text'])
                print(f"Debug: Chunk pertinent trouv√©: ID={global_faiss_id}")
            else:
                print(f"Attention : M√©tadonn√©es ou texte du chunk introuvables pour l'ID Faiss global {global_faiss_id}")

        if not relevant_chunks_texts:
            print("Aucun chunk pertinent trouv√© (seuil de similarit√© non atteint ou probl√®me de m√©tadonn√©es).")
            return None
        return relevant_chunks_texts

    # ... (autres m√©thodes : clean_llm_response, generate_response, etc. restent inchang√©es) ...
    def clean_llm_response(self, response: str) -> str:
    # Extraire uniquement le texte apr√®s la derni√®re occurrence de '</'
        if '</' in response:
            response = response.split('</')[-1]

        # Nettoyage suppl√©mentaire
        cleaned = re.sub(r"<?think>", "", response, flags=re.DOTALL | re.IGNORECASE)
        cleaned = re.sub(r"</?think>", "", cleaned, flags=re.IGNORECASE)
        return cleaned.strip()
    
    SQLITE_DB_PATH = "./bdd/chatbot_metadata.db"
    
    def generate_response(self, user_query, user_id, profile_id, departement_id, filiere_id, use_mmr=False):
        INVITE_PROFILE_ID = 5

        is_invite = (profile_id == INVITE_PROFILE_ID)
        if is_invite:
            departement_id = 1
            filiere_id = 3

        context_chunks = self.find_relevant_context(
            user_query, departement_id, filiere_id, top_k=3, similarity_threshold=0.65, use_mmr=use_mmr
        )

        if context_chunks:
            context_text = "\n".join(context_chunks)
            if PromptBuilder.is_qcm_request(user_query):
                prompt_text = PromptBuilder.build_qcm_prompt(context_text, user_query)
            elif "r√©sum√©" in user_query.lower() or "synth√®se" in user_query.lower():
                prompt_text = PromptBuilder.build_summary_prompt(context_text, user_query)
            else:
                prompt_text = PromptBuilder.build_standard_prompt(context_text, user_query)

            # Ajout de la consigne sp√©ciale pour l'invit√©
            if is_invite:
                prompt_text = (
                    "‚ö†Ô∏è **Mode invit√© activ√© :**\n"
                    "Tu dois r√©pondre **uniquement** √† partir des informations fournies par le d√©partement Scolarit√©.\n"
                    "Si la question ne concerne pas le d√©partement Scolarit√©, ou si tu n‚Äôas pas l‚Äôinformation dans le contexte, r√©ponds exactement :\n"
                    "\"Je ne peux r√©pondre qu‚Äôaux questions relatives au d√©partement Scolarit√©.\"\n"
                    "N‚Äôajoute rien d‚Äôautre, n‚Äôexplique pas, ne propose pas de ressources, ne donne aucune information suppl√©mentaire.\n"
                    "N‚Äôutilise **aucune autre information**, m√™me si elle est pr√©sente dans le contexte.\n"
                    "N‚Äôint√®gre pas de ressources suppl√©mentaires ext√©rieures au d√©partement Scolarit√©.\n\n"
                    "**Gestion des Ressources :**\n"
                    "La section ¬´ üìö Pour Aller Plus Loin ¬ª doit appara√Ætre **uniquement si des ressources pertinentes sont disponibles** dans le contexte du d√©partement Scolarit√©.\n"
                    "Si aucune ressource n‚Äôest pertinente, **n‚Äôinclus pas cette section**.\n\n"
                    "**Important :**\n"
                    "Ne r√©ponds √† aucune question qui ne concerne pas le d√©partement Scolarit√©. Ignore toute demande hors de ce p√©rim√®tre.\n"
                    "Exemple :\n"
                    "Q : Qu‚Äôest-ce que PySpark ?\n"
                    "R : Je ne peux r√©pondre qu‚Äôaux questions relatives au d√©partement Scolarit√©.\n"
                ) + prompt_text

        else:
            prompt_text = (
                f"Question : {user_query}\n"
                "R√©ponds que tu n'as pas d'informations pertinentes pour cette question."
            )

        llm_raw_response = self.ollama_api.chat_with_ollama(prompt_text)
        cleaned_llm_response = self.clean_llm_response(llm_raw_response)

        chat_id = FilterManager.save_chat_history(
            user_id=user_id,
            question=user_query,
            answer=cleaned_llm_response,
            departement_id=departement_id,
            filiere_id=filiere_id,
            profile_id=profile_id,
            db_path=self.SQLITE_DB_PATH
        )

        return cleaned_llm_response, chat_id

    # def generate_response(self, user_query,user_id,profile_id, departement_id, filiere_id):
    #     # Appelle votre m√©thode pour trouver le contexte pertinent
    #     context_chunks = self.find_relevant_context(
    #         user_query, departement_id, filiere_id, top_k=3, similarity_threshold=0.55
    #     )

    #     prompt_text = "" # Initialise la variable pour le texte de l'invite

    #     if context_chunks:  # Si context_chunks n'est pas None et n'est pas une liste vide
    #         context_text_joined = "\n".join(context_chunks)
    #         #print(context_chunks)
    #         prompt_text = (
    #             f"Contexte :\n{context_text_joined}\n\n"
    #             f"Question : {user_query}\n"
    #                 """
    #                 vous √™tes un assistant p√©dagogique. R√©ponds uniquement √† partir du contexte fourni ci-dessus, sans ajouter, inf√©rer ni reformuler d‚Äôinformations ext√©rieures.
    #                 Le cadre est acad√©mique‚ÄØ: adopte un ton engageant et motivant pour l‚Äô√©tudiant, tout en restant clair, pr√©cis et inspirant.
    #                 Fournis une explication concise, sans introduction, justification ou r√©p√©tition superflue.
    #                 Supprime toute balise <...> dans la r√©ponse.
    #                 Exprime-toi uniquement en fran√ßais.
    #                 Structure la r√©ponse selon la charte suivante‚ÄØ:

    #                 ********************
    #                 R√©ponse :
    #                 [Une phrase claire, pr√©cise et inspirante, directement issue du contexte.]
    #                 ********************

    #                 Ressources suppl√©mentaires (si disponibles) :
    #                 - [Titre de la ressource 1] : [URL]
    #                 - [Titre de la ressource 2] : [URL]
    #                 ********************

    #                 N‚Äôaffiche la section ¬´‚ÄØRessources suppl√©mentaires‚ÄØ¬ª que si des ressources pertinentes sont disponibles.
    #                 Ne fournis jamais d‚Äôinformations non pr√©sentes dans le contexte.

    #                 *********************
    #                 Si l'etudiant demande un QCM proc√®de √† ce qui suit : En te basant sur ce contexte extrait des documents de cours,
    #                 g√©n√®re un QCM li√© √† la question suivante.

    #                 Contexte : 
    #                 {context_chunks}

    #                 Question : {user_query}

    #                 G√©n√®re une liste des questions QCM avec 4 propositions selon le nombre demand√© par l'√©tudiant mais ne pas d√©passer 10 au maximum, indique clairement la bonne r√©ponse.

    #                 Format :

    #                 Question : [ta question]
    #                 1) choix A
    #                 2) choix B
    #                 3) choix C
    #                 4) choix D

    #                 R√©ponse correcte : [num√©ro ou texte]
    #                 """
    #         )
    #     else:
    #         # Ceci est l'invite de votre bloc 'else' original
    #         prompt_text = (
    #             f"Question : {user_query}\n"
    #             "Si la question est hors le cadre acad√©mique, r√©pond par : je n'ai pas la capacit√© de r√©pondre √† votre question.\n\nR√©ponse :"
    #         )
        
    #     llm_raw_response = self.ollama_api.chat_with_ollama(prompt_text)
    #     cleaned_llm_response = self.clean_llm_response(llm_raw_response)

    #     FilterManager.save_chat_history(
    #         user_id=user_id, question=user_query, answer=cleaned_llm_response,
    #         departement_id=departement_id, filiere_id=filiere_id, profile_id=profile_id
    #     )
    #     return cleaned_llm_response

    def simulate_typing(self, text, delay=0.009):
        for char in text:
            sys.stdout.write(char)
            sys.stdout.flush()
            time.sleep(delay)
        print()

    def format_response(self, response):
        formatted_response = ""
        for char in response:
            if char.isdigit():
                formatted_response += Fore.BLUE + char + Style.RESET_ALL
            elif char == '`':
                formatted_response += Fore.GREEN + char + Style.RESET_ALL
            else:
                formatted_response += char
        return formatted_response

    def display_welcome_message(self):
        welcome_message = Fore.GREEN + "Chatbot: Bonjour ! Tapez 'exit' pour quitter." + Style.RESET_ALL
        self.simulate_typing(welcome_message)

    def display_exit_message(self):
        exit_message = Fore.RED + "Chatbot: Au revoir ! √Ä bient√¥t !" + Style.RESET_ALL
        self.simulate_typing(exit_message)

    def chat(self, departement_id, filiere_id, module_id, activite_id, profile_id, user_id):
        self.display_welcome_message()
        while True:
            user_input = input("\n" + Fore.YELLOW + "Vous : " + Style.RESET_ALL)
            if user_input.lower() == "exit":
                self.display_exit_message()
                break
            try:
                response = self.generate_response(user_input, departement_id, filiere_id, module_id, activite_id, profile_id, user_id)
                formatted_response = self.format_response(response)
                self.simulate_typing(Fore.CYAN + f"Chatbot : {formatted_response}" + Style.RESET_ALL)
            except Exception as e:
                self.simulate_typing(Fore.RED + f"Chatbot : D√©sol√©, une erreur s'est produite : {str(e)}" + Style.RESET_ALL)